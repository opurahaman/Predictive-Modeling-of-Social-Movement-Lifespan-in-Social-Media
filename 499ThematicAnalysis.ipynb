{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOok6X7JIyl",
        "outputId": "5ad686f9-64bc-4bf0-ed83-569f9ca66e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:\n",
            "codicymru black death racism solidarity aberystwyth history opportunity harris lives\n",
            "Topic 1:\n",
            "black blm blazetv fearless blacklivesmatter basketball lives matter jasonwhitlock jordan\n",
            "Topic 2:\n",
            "blacklivesmatter blackwellness black bana bantu stacey keys skeys house beauty\n",
            "Topic 3:\n",
            "black lives matter blacklivesmatter people blm make blackhistory answer music\n",
            "Topic 4:\n",
            "chicago children lives viral suicide freedom blacklivesmatter thing police black\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text data.\"\"\"\n",
        "    text = text.lower()  #lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  #single space\n",
        "    text = re.sub(r'http\\S+', '', text)  # RemoveURLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove other characters (alphabets chara)\n",
        "    return text\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    \"\"\"Display the topics from an LDA model.\"\"\"\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_path = '/content/BlackLivesMatter.csv'  # Database path ekhane paste korba\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    data = data.dropna(subset=['posts'])\n",
        "\n",
        "    data['posts'] = data['posts'].apply(preprocess_text)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    tfidf = tfidf_vectorizer.fit_transform(data['posts'])\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(tfidf)\n",
        "\n",
        "    display_topics(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}